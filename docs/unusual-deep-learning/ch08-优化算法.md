# 八 优化算法

## 1 梯度下降（GD）

- **算法**： 
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+w_%7B0%7D+%26%3D%5Ctext+%7B+initializaiton+%7D+%5C%5C+w_%7Bt%2B1%7D+%26%3Dw_%7Bt%7D-%5Ceta+%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29+%5Cend%7Baligned%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 称为步长或学习率。

- **核心思想**:找出局部最陡的梯度下降方向 
![[公式]](https://www.zhihu.com/equation?tex=-%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29)

- **原理**：在下降梯度和足够小的学习率下，优化函数值总是减小，除非迭代处的梯度为零

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%5Ctext+%7B+Lemma+%28Descent+Lemma%29.+Suppose+%7D+f+%5Ctext+%7B+is+%7D+L+%5Ctext+%7B+-smooth.+Then%2C+if+%7D+%5Ceta%3C+%5C%5C+1+%2F%282+L%29%2C+%5Ctext+%7B+we+have+%7D+%5C%5C+%5Cqquad+f%5Cleft%28w_%7Bt%2B1%7D%5Cright%29+%5Cleq+f%5Cleft%28w_%7Bt%7D%5Cright%29-%5Cfrac%7B%5Ceta%7D%7B2%7D+%5Ccdot%5Cleft%5C%7C%5Cnabla+f%5Cleft%28w_%7Bt%7D%5Cright%29%5Cright%5C%7C_%7B2%7D%5E%7B2%7D%5Cend%7Barray%7D)

## 2 随机梯度下降（SGD）
减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引 ![[公式]](https://www.zhihu.com/equation?tex=i%5Cin%5C%7B1%2C%5Cldots%2Cn%5C%7D) ，并计算梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f_i%28%5Cboldsymbol%7Bx%7D%29) 来迭代 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D)。每次迭代的计算开销从梯度下降的O(n)降到了常数O(1)

## 3 加速梯度下降（AGD）
- 动量法优化梯度下降
- 指数加权移动平均

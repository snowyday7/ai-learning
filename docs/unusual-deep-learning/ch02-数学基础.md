# 二 数学基础

## 1 矩阵论

- **张量**：标量是0阶张量，矢量是1阶张量，矩阵是2阶张量，三维及以上数组称为张量
- **矩阵的秩**（Rank）：矩阵向量中的极大线性无关组的数目
- **矩阵的逆**：
  1. 奇异矩阵：$rank(A_{n×n})<n$
  2. 非奇异矩阵：$rank(A_{n×n})=n$
- **广义逆矩阵**：如果存在矩阵$B$使得$ABA=A$，则称$B$为$A$的广义逆矩阵
- **矩阵分解**：
  1. 特征分解：$A = U\Sigma U^{T}$
  2. 奇异值分解：$A = U \Sigma V^{T}$、$U^T U = V^T V = I$

## 2 概率统计

- **随机变量**：
  1. 分类：离散随机变量、连续随机变量
  2. 概念：用概率分布来指定它的每个状态的可能性
  
- **常见的概率分布**：
  1. 伯努利分布：单个二值型离散随机变量的分布，概率分布函数：$P(X=1)=p,P(X=0)=1-p$
  2. 二项分布：重复$n$次伯努利试验，概率分布函数：$P(X = k) = C_n^k p^k (1-p)^{n-k}$
  3. 均匀分布：概率密度函数：$\displaystyle p(x) = \frac{1}{b-a}, \quad a < x <b$
  4. 高斯分布：又称正态分布，概率密度函数：$\displaystyle p(x) = \frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$
  5. 指数分布：独立随机事件发生的时间间隔，概率密度函数：$p(x) = \lambda e^{-\lambda x} (x \geqslant 0)$

- **多变量概率分布**：
  1. 条件概率：$P(X | Y)$
  2. 联合概率：$P(X, Y)$ 、 $​P(Y|X) = \frac{P(Y,X)}{P(X)} \quad P(X ) > 0$
  3. 先验概率：在事件发生前已知的概率，“由因求果”
  4. 后验概率：基于新的信息，修正后来的先验概率，获得更接近实际情况的概率估计，“执果寻因”
  5. 全概率公式：$\displaystyle P(B) = \sum_{i = 1}^nP(A_i)P(B|A_i)$
  6. 贝叶斯公式：
  $$
  P(A_i | B) 
  = \frac{ P(B | A_i) P(A_i)}{P(B)} 
  = \frac{P(B | A_i) P(A_i)} {\displaystyle \sum_{j=1}^{n} P(A_j) P(B | A_j)}
  $$

- **常用统计量**：
  1. 方差：随机变量与数学期望之间的偏离程度
  $\text{Var}(X) = E\left\{ [x-E(x)]^2 \right \} = E( x^2 ) -[E(x)]^2$
  2. 协方差：两个随机变量$X$和$Y$的总体误差
  $\text{Cov}(X,Y)=E\left\{ [x-E(x)][y-E(y)] \right\}=E \left( xy \right) - E(x)E(y)$

## 3 信息论

- **熵**：信息熵，可以看作是样本集合纯度一种指标，也可以认为是样本集合包含的平均信息量

假定当前样本集合X中第*i*类样本 $𝑥_𝑖$ 所占的比例为$P(𝑥_𝑖)(i=1,2,...,n)$，则*X*的信息熵定义为：

$$
H(X) = -\sum_{i = 1}^n P(x_i)\log_2P(x_i)
$$

H(X)的值越小，则X的纯度越高，蕴含的不确定性越少

- **联合熵**：两个随机变量X和Y的联合分布可以形成联合熵，度量二维随机变量XY的不确定性：
$$
H(X, Y) = -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j)\log_2 P(x_i,y_j)
$$

- **条件熵**：在随机变量X发生的前提下，随机变量Y发生带来的熵，定义为Y的条件熵，用H(Y|X)表示，定义为：

$$
H(Y|X) = \sum_{i = 1}^n P(x_i)H(Y|X = x_i)
= -\sum_{i = 1}^n P(x_i) \sum_{j = 1}^n P(y_j|x_i)\log_2 
P(y_j|x_i)
= -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j) \log_2 
P(y_j|x_i)
$$

熵、联合熵和条件熵之间的关系：

$$
H(Y|X) = H(X,Y)-H(X)
$$

- **互信息**

$$
I(X;Y) = H(X)+H(Y)-H(X,Y)
$$

- **相对熵**：相对熵又称KL散度，是描述两个概率分布P和Q差异的一种方法，记做D(P||Q)
  - 离散形式：$D(P||Q) = \sum P(x)\log \frac{P(x)}{Q(x)}$.
  - 连续形式：$D(P||Q) = \int P(x)\log \frac{P(x)}{Q(x)}$.

- **交叉熵**：一般用来求目标与预测值之间的差距，深度学习中经常用到的一类损失函数度量

$$
H(P,Q) = -\sum P(x)\log Q(x)
$$

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 三 机器学习基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 基本概念\n",
    "\n",
    "- **概念**：从已知数据中获得规律，并利用规律对未知数据进行预测\n",
    "- **分类**：有监督学习(“分类”和“回归”)、无监督学习(“聚类”和“降维”)、强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数据集\n",
    "\n",
    "- **概念**：观测样本的集合，$D=\\{x_1, x_2, \\dots, x_n\\}$，其中$d$为样本空间的维度。其中 $𝑥_𝑖$ 是一个向量，表示数据集的第𝑖个样本，其维度𝑑称为样本空间的维度。向量 $𝑥_𝑖$ 的元素称为样本的特征，其取值可以是连续的，也可以是离散的。从数据集中学出模型的过程，便称为“学习”或“训练”。\n",
    "- **分类**：训练集、验证集、测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "误差是指算法实际预测输出与样本真实输出之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 过拟合与欠拟合\n",
    "\n",
    "- **误差**：训练误差、泛化误差、测试误差\n",
    "- **欠拟合**：`高偏差低方差`。模型没有很好的训练出数据的一般规律，模型拟合程度不高\n",
    "  - 寻找更好的特征\n",
    "  - 增加特征数量\n",
    "  - 调整参数\n",
    "  - 增加迭代深度\n",
    "  - 用更加复杂的模型\n",
    "- **过拟合**：`低偏差高方差`。能很好拟合训练样本，而无法很好拟合测试样本，导致泛化性能下降\n",
    "  - 增加训练样本数量\n",
    "  - 减少特征数量维数\n",
    "  - 减少参数\n",
    "  - 降低模型复杂度\n",
    "  - 正则化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 泛化误差分析\n",
    "\n",
    "- **误差公式**：$$\\text{Err}(\\hat{f})=\\text{Bias}^2 (\\hat{f})+\\text{Var}(\\hat{f})+\\sigma_{\\varepsilon}^2$$其中$\\varepsilon \\sim N(0,\\sigma_{\\varepsilon})$时噪声，潜在模型为$Y=f(x) + \\varepsilon$，估计模型为$\\hat{f}(X)$\n",
    "- **偏差**：反映模型在样本上的期望输出与真实标记之间的差距，即模型精准度，反映模型的拟合能力\n",
    "- **方差**：反映模型在不同训练集下学得的函数的输出与期望输出之间的误差，即模型稳定性，反映模型的波动情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 交叉验证\n",
    "\n",
    "- **基本思路**：$K$折交叉验证，将训练集划分为$K$份，每次采用其中$K$-1份作为训练集， 另外一份作为验证集，在训练集上学得函数后，然后在验证集上计 算误差\n",
    "- **交叉验证方法**：$K$折交叉验证、留一交叉验证、$K$折重复交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 有监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据集**有标记**(答案)\n",
    "- 数据集通常扩展为$(𝑥_𝑖,𝑦_𝑖)$\n",
    "- **学习任务**：已知输出空间，训练一个模型用于预测$y$的取值，使得$f(x) \\cong y$\n",
    "- **分类**：预测值是离散值\n",
    "- **回归**：预测值是连续值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 线性回归\n",
    "\n",
    "- **概念**：线性回归是在样本属性和标签中找到一个线性关系的方法，训练一个线性模型，使得预测值与样本标签的差值最小\n",
    "- **公式**：$$\\displaystyle f(x^{(k)}) = \\sum_{i=1}^m w_i x_i^{(k)} + b$$\n",
    "- **优化函数**：$$\\displaystyle (w^*,b^*) =\\mathop{\\arg\\min} \\limits_{(w,b)} \\sum_{k = 1}^n(w^T x^{(k)}+b-y^{(k)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 逻辑回归\n",
    "\n",
    "- **概念**：利用sigmoid函数，将线性回归产生的预测值映射为0到1之间\n",
    "- **公式**：\n",
    "$$\n",
    "g(f(x^{(k)}))=\\left\\{\n",
    "\\begin{array}{l}\n",
    "1, \\frac{1}{\\displaystyle 1+e^{-(w^T x^{(k)}+b)}}\\geq 0.5 \\\\ \n",
    "0, \\text{otherwise}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 支持向量机\n",
    "\n",
    "- **SVM基本思想**：对于线性可分的数据，找到一个位于两类训练样本正中心的超平面，使得margin最大化\n",
    "- **线性分类优化函数**：\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\displaystyle & \\mathop{\\arg \\min} \\limits_{(w,b)} \\frac{1}{2}\\sum_{i = 1}^d w_i^2 \\\\ \n",
    "s.t. & \\forall x_i \\in D, |w^T x_i + b| \\geq 1\n",
    "\\end{array}$$\n",
    "- **非线性分类优化函数**：\n",
    "  - 特征空间存在超曲面(hypersurface)将正类和负类分开\n",
    "  - `核函数`：\n",
    "    - 使用非线性函数将非线性可分问题从原始的特征空间映射至更高维\n",
    "    - 决策边界的超平面表示为 $w^T \\phi(x)+b = 0$\n",
    "    - 定义映射函数的内积为核函数 $K(X_i,x_j) = \\phi(x_i)^T \\phi(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 决策树\n",
    "\n",
    "- **概念**：基于树结构进行决策的机器学习方法，树结构中，叶子节点给出类别，内部节点代表某个属性\n",
    "- **树的生成**：最重要的因素便是根节点的选择，即选择哪种特征作为决策因素。ID3算法使用信息增益作为准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 随机森林\n",
    "\n",
    "- **基本思路**：创建多棵决策树，组成一个森林，且每棵决策树之间无关联，根据多数原则决定样本分类结果\n",
    "- **集成学习**：组合多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来\n",
    "- **构建步骤**：\n",
    "  - 随机有放回从训练集中抽取$m$个训练样本，创建训练集\n",
    "  - 随机选择部分特征，创建决策树\n",
    "  - 重复上述步骤构建多棵决策树\n",
    "- **预测步骤**：\n",
    "  - 向建立好的随机森林中输入一个新样本\n",
    "  - 随机森林中的每棵决策树都独立的做出判断\n",
    "  - 将得到票数最多的分类结果作为该样本最终的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据集**没有标记**信息(自学)\n",
    "- **聚类**：把关联度大的样本划为同一类，关联度小的样本划为不同类\n",
    "- **降维**：把维度较高、计算复杂的数据，转化为维度低、易处理、且蕴含的信息不丢失或较少丢失的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 聚类\n",
    "\n",
    "- **基本思路**：将无标签数据分为多个类别，相似的归为一类，不相似的归为其它类\n",
    "- **常见方法**：K-Means聚类、均值漂移聚类、基于密度的聚类\n",
    "- **K-Means**聚类算法步骤：\n",
    "  1. 选取$K$个对象作为初始聚类中心\n",
    "  2. 计算样本数据与聚类中心的欧式距离，按距离最近原则将数据分类到最近的聚类中心\n",
    "  3. 更新聚类中心，计算每个类别所有对象的均值，将其作为新的聚类中心，计算目标函数的值\n",
    "  4. 判断聚类中心和目标函数的值是否发生变化，没有变化则输出聚类结果，否则返回步骤2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 降维\n",
    "\n",
    "- **基本思路**：将样本数据从维度$d$降低到更小的维度$m$，且尽量使样本蕴含的信息量损失最小或还原时的误差最小\n",
    "- **常见方法**：PCA主成分分析法\n",
    "- **优势**：\n",
    "  1. 数据在低纬度下更容易处理\n",
    "  2. 更能显示重要特征\n",
    "  3. 方便进行可视化展示\n",
    "  4. 去除数据噪声，优化算法资源"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

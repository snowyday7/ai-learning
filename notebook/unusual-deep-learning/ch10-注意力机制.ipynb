{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 机器翻译Encoder-Decoder\n",
    "\n",
    "- 神经机器翻译主要以`Encoder-Decoder`模型为基础结构\n",
    "![4](./images/ch10/4.png)\n",
    "\n",
    "- 在神经机器翻译中，**Encoder**一般采用`RNN`或者`LSTM`实现\n",
    "  - 从统计角度，翻译相当于寻找译句 $y$，使得给定原句 $x$ 时条件概率最大：$\\arg\\max_{y} p(\\boldsymbol{y} | \\boldsymbol{x})$\n",
    "  - 得到上下文向量 **$c$** 的方法有很多，可以直接将最后一个隐状态作为上下文变量，也可对最后的隐状态进行一个非线性变换 $\\sigma(\\cdot)$，或对所有的隐状态进行非线性变换 $\\sigma(\\cdot)$.\n",
    "\n",
    "$$\n",
    "c =h_{T} \\\\  \\,\\,\\,\\  c =\\sigma\\left(h_{T}\\right) \\\\ c =\\sigma\\left(h_{1}, h_{2}, \\cdots, h_{T}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 解码器\n",
    "- 用给定的上下文向量 $c$ 和之前已经预测的词 $\\{y_1,\\cdots,y_{t-1}\\}$预测$y_t$\n",
    "\n",
    "![5](./images/ch10/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 现存问题\n",
    "\n",
    "- 输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示\n",
    "- 这个问题限制了模型的性能，尤其是当输入序列比较长时，模型的性能会变得很差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 神经网络模型注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](./images/ch10/8.png)\n",
    "\n",
    "- 在这个新结构中，定义每个输出的条件概率为:  $p\\left(y_{i} \\mid y_{1}, \\cdots, y_{i-1}, \\boldsymbol{x}\\right)=g\\left(y_{i-1}, x_{i}, c_{i}\\right)$.\n",
    "- 其中 $s_i$ 为解码器RNN中的隐层状态: $s_{i}=f \\left(s_{i-1}, y_{i-1}, c_{i}\\right)$.\n",
    "- 这里的上下文向量 $c_i$ 取决于注释向量序列 (encoder转化得到)，通过使用注意力系数 $\\alpha_{ij}$ 对 $h_j$  加权求得：\n",
    "\n",
    "$$\n",
    "c_{i}=\\sum_{j=1}^{T} \\alpha_{i j} h_{j}\n",
    "$$\n",
    "\n",
    "![9](./images/ch10/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 注意力机制的计算\n",
    "\n",
    "- **注意力系数计算**\n",
    "\n",
    "$$\n",
    "\\alpha_{i j}=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k=1}^{T_{x}} \\exp \\left(e_{i k}\\right)} \\quad e_{i j}=a\\left(s_{i-1}, h_{j}\\right)\n",
    "$$\n",
    "\n",
    "后一个公式中的 $a(\\cdot)$ 表示alignment mode，反映 $i$ 位置的输入和 $j$ 位置输出的匹配程度。\n",
    "\n",
    "- 计算注意力系数的**相似函数**(alignment model)有以下几种：\n",
    "\n",
    "$$\n",
    "a\\left(s_{i-1}, h_{j}\\right) = \n",
    "h_{j}^{T} \\cdot s_{i-1} \\\\\n",
    "a\\left(s_{i-1}, h_{j}\\right) = \n",
    "\\frac{h_{j}^{T} \\cdot W_{\\alpha} \\cdot s_{i-1}}{W_{\\alpha} \\cdot\\left[h_{j}^{T}, s_{i-1}^{T}\\right]^{T}} \\\\\n",
    "a\\left(s_{i-1}, h_{j}\\right) = \n",
    "v_{\\alpha} \\tanh \\left(U_{\\alpha} h_{j}+W_{\\alpha} s_{i-1}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 几种主流的注意力机制\n",
    "\n",
    "![6](./images/ch10/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 注意力机制的理解\n",
    "\n",
    "- `Attention函数`的**本质**：可以被描述为一个查询(query)到一系列(键key-值value)对的映射\n",
    "\n",
    "![7](./images/ch10/7.png)\n",
    "\n",
    "$$\n",
    "\\text { Attention(Query, Source })=\\sum_{i=1}^{L_{x}} \\text { similarity }\\left(\\text { Query }, \\mathrm{Key}_{i}\\right){\\times} \\text { Value }_{i}\n",
    "$$\n",
    "\n",
    "- **注意力系数计算**\n",
    "  - 阶段1：根据Query和Key计算两者的相似性或者相关性\n",
    "  - 阶段2：对第一阶段的原始分值进 行归一化处理\n",
    "  - 阶段3：根据权重系数对Value进行加权求和，得到Attention Value\n",
    "\n",
    "![12](./images/ch10/12.png)\n",
    "\n",
    "- **Self-attention layer** in “Attention is all you need”>\n",
    "\n",
    "![13](./images/ch10/13.png)\n",
    "\n",
    "![14](./images/ch10/14.png)\n",
    "\n",
    "![15](./images/ch10/15.png)\n",
    "\n",
    "![16](./images/ch10/16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 注意力机制的应用\n",
    "\n",
    "- **GPT/GPT-2**\n",
    "- **BERT**\n",
    "\n",
    "![17](./images/ch10/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 记忆网络\n",
    "\n",
    "### 2.1 代表工作\n",
    "\n",
    "- **Memory Networks**. ICLR 2015\n",
    "- **End-To-End Memory Networks**. NIPS 2015: 2440-2448\n",
    "- **Key-Value Memory Networks for Directly Reading Documents**. EMNLP 2016: 1400-1409\n",
    "- **Tracking the World State with Recurrent Entity Networks**. ICLR 2017"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

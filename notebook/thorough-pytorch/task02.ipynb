{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task02 PyTorch基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 张量tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 概念\n",
    "\n",
    "- 概念：一个数据容器，可以包含数据、字符串等\n",
    "- 分类：0维张量（标量）、1维张量（向量）、2维张量（矩阵）、3维张量（时间序列）、4维张量（图像）、5维张量（视频）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0479, 0.8201, 0.5482],\n",
      "        [0.4645, 0.2504, 0.5394],\n",
      "        [0.8791, 0.0699, 0.1394],\n",
      "        [0.0695, 0.9159, 0.1081]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建tensor\n",
    "x = torch.rand(4, 3)\n",
    "print(x)\n",
    "# 构造数据类型为long，数据是0的矩阵\n",
    "x = torch.zeros(4, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 常见的构造Tensor的方法：\n",
    "\n",
    "|                  函数 | 功能                                                |\n",
    "| --------------------: | --------------------------------------------------- |\n",
    "|      Tensor(sizes) | 基础构造函数                                        |\n",
    "|        tensor(data) | 类似于np.array                                      |\n",
    "|        ones(sizes) | 全1                                                 |\n",
    "|       zeros(sizes) | 全0                                                 |\n",
    "|         eye(sizes) | 对角为1，其余为0                                    |\n",
    "|    arange(s,e,step) | 从s到e，步长为step                                  |\n",
    "| linspace(s,e,steps) | 从s到e，均匀分成step份                              |\n",
    "|  rand/randn(sizes) | rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布 |\n",
    "|    normal(mean,std) | 正态分布(均值为mean，标准差是std)                 |\n",
    "|         randperm(m) | 随机排列                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法、索引、维度变换、取值等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1874, 0.1504, 0.2045],\n",
      "        [0.4453, 0.9457, 0.4930],\n",
      "        [0.2341, 0.2106, 0.7255],\n",
      "        [0.3411, 0.8462, 0.7029]])\n",
      "tensor([[0.1874, 0.1504, 0.2045],\n",
      "        [0.4453, 0.9457, 0.4930],\n",
      "        [0.2341, 0.2106, 0.7255],\n",
      "        [0.3411, 0.8462, 0.7029]])\n",
      "tensor([[0.1874, 0.1504, 0.2045],\n",
      "        [0.4453, 0.9457, 0.4930],\n",
      "        [0.2341, 0.2106, 0.7255],\n",
      "        [0.3411, 0.8462, 0.7029]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 方式1\n",
    "y = torch.rand(4, 3) \n",
    "print(x + y)\n",
    "\n",
    "# 方式2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 方式3 in-place，原值修改\n",
    "y.add_(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用索引表示的变量与原数据共享内存，可使用copy()等方法改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2408, 0.9194, 0.8660, 0.6691])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(4,3)\n",
    "# 取第二列\n",
    "print(x[:, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 维度变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的维度变换常见的方法有torch.view()和torch.reshape()。torch.view()会改变原始张量的维度，而torch.reshape()会返回一个新张量，但是此函数并不能保证返回的是其拷贝值。推荐的方法是我们先用 clone() 创造一个张量副本然后再使用 torch.view()进行函数维度变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4]) torch.Size([20]) torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 使用view改变张量的大小\n",
    "x = torch.randn(5, 4)\n",
    "y = x.view(20)\n",
    "z = x.view(-1, 5) # -1是指这一维的维数由其他维度决定\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有一个元素 tensor ，我们可以使用 .item() 来获得这个 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1) \n",
    "print(type(x)) \n",
    "print(type(x.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[1, 2]])\n",
      "y = tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "x + y = tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"x + y =\", x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `autograd`包：为张量上的操作提供自动求导机制\n",
    "- torch.Tensor类：如果设置`.requires_grad`为`True`，那么将会追踪张量的所有操作。当完成计算后，可以通过调用`.backward()`自动计算所有的梯度。张量的所有梯度将会自动累加到`.grad`属性。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中\n",
    "- `Function`：`Tensor`和`Function`互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，该属性引用了创建`Tensor`自身的`Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>)\n",
      "z mean =  tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(\"z = \", z)\n",
    "print(\"z mean = \", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度：对于那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵\n",
    "$$\n",
    "J=\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `grad`的反向传播：运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播累加\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 梯度清零\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1591, -0.6704,  1.0540], requires_grad=True)\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目的：通过使用多个GPU参与训练，加快训练速度，提高模型学习的效果\n",
    "- CUDA：是NVIDIA提供的一种GPU并行计算框架，采用`.cuda()`方法是让模型或者数据迁移到GPU中进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意：\n",
    "    1. 数据在GPU和CPU之间进行传递时会比较耗时，尽量避免数据的切换。\n",
    "    2. GPU运算很快，但是在使用简单的操作时，我们应该尽量使用CPU去完成。\n",
    "    3. tensor.cuda()方法会默认将tensor保存到第一块GPU上，等价于tensor.cuda(0)，这将有可能导致爆出out of memory的错误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置在文件最开始部分\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"] = \"2\" # 设置默认的显卡\n",
    "\n",
    "#或者\n",
    "CUDA_VISBLE_DEVICE=0,1  # 使用0，1两块GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 并行计算方法：\n",
    "  1. Network partitioning：将一个模型网络的各部分拆分，分配到不同的GPU中,执行不同的计算任务\n",
    "  2. Layer-wise partitioning：将同一层模型拆分，分配到不同的GPU中，训练同一层模型的部分任务\n",
    "  3. Data parallelism（主流）：将不同的数据分配到不同的GPU中，执行相同的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 使用CUDA加速训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 单卡训练\n",
    "```python\n",
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "for image,label in dataloader:\n",
    "\n",
    "    # 图像和标签显示转移到CUDA上    \n",
    "    image = image.cuda() \n",
    "    label = label.cuda()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 多卡训练\n",
    "PyTorch提供了两种多卡训练的方式，分别为DataParallel和DistributedDataParallel（以下我们分别简称为DP和DDP）。DDP性能更好，但更复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 单机多卡DP\n",
    "主要使用数据并行nn.DataParallel函数，比较简单\n",
    "```python\n",
    "model = Net()\n",
    "model.cuda() # 模型显示转移到CUDA上\n",
    "\n",
    "if torch.cuda.device_count() > 1: # 含有多张GPU的卡\n",
    "\tmodel = nn.DataParallel(model) # 单机多卡DP训练\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以指定GPU进行并行训练，一般有两种方式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#nn.DataParallel函数传入device_ids参数，可以指定了使用的GPU编号\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "#要手动指定对程序可见的GPU设备\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 多机多卡DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 进程组的相关概念\n",
    "\n",
    "- GROUP：进程组，默认情况下，只有一个组，一个 job 即为一个组，也即一个 world。（当需要进行更加精细的通信时，可以通过 new_group 接口，使用 world 的子集，创建新组，用于集体通信等。）\n",
    "\n",
    "- WORLD_SIZE：表示全局进程个数。如果是多机多卡就表示机器数量，如果是单机多卡就表示 GPU 数量。\n",
    "\n",
    "- RANK：表示进程序号，用于进程间通讯，表征进程优先级。rank = 0 的主机为 master 节点。 如果是多机多卡就表示对应第几台机器，如果是单机多卡，由于一个进程内就只有一个 GPU，所以 rank 也就表示第几块 GPU。\n",
    "\n",
    "- LOCAL_RANK：表示进程内，GPU 编号，非显式参数，由 torch.distributed.launch 内部指定。例如，多机多卡中 rank = 3，local_rank = 0 表示第 3 个进程内的第 1 块 GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DDP的基本用法 (代码编写流程)\n",
    "\n",
    "- 在使用 distributed 包的任何其他函数之前，需要使用 init_process_group 初始化进程组，同时初始化 distributed 包。\n",
    "\n",
    "- 使用 torch.nn.parallel.DistributedDataParallel 创建 分布式模型 DDP(model, device_ids=device_ids)\n",
    "\n",
    "- 使用 torch.utils.data.distributed.DistributedSampler 创建 DataLoader\n",
    "\n",
    "- 使用启动工具 torch.distributed.launch 在每个主机上执行一次脚本，开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是对代码进行修改，添加参数 --local_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int) # 这个参数很重要\n",
    "args = parser.parse_args()\n",
    "torch.cuda.set_device(args.local_rank) # 调整计算的位置，在所有与GPU相关代码前设置\n",
    "\n",
    "# ps 检查nccl是否可用\n",
    "# torch.distributed.is_nccl_available ()\n",
    "torch.distributed.init_process_group(backend='nccl') # 选择nccl后端，初始化进程组\n",
    "\n",
    "# 创建Dataloader\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=train_sampler)\n",
    "\n",
    "# 创建DDP模型，DDP进行训练\n",
    "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#启动DDP\n",
    "```python\n",
    "  CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py\n",
    "  # nproc_per_node: 这个参数是指你使用这台服务器上面的几张显卡\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次任务，主要介绍了PyTorch概念及优势、以及基础知识，包括张量、自动求导和并行计算；通过构建张量，存储我们需要的数据；基于自动求导机制和雅可比矩阵的计算规则，计算张量的梯度；并行计算方法主要包括Network partitioning、Layer-wise partitioning和Data parallelism，目前主流的是最后一种。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
